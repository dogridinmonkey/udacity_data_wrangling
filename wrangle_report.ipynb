{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84140da3",
   "metadata": {},
   "source": [
    "## Report: wrangle_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3272aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://piximus.net/media2/61646/weratedogs-tweets-26.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Image(url=\"https://piximus.net/media2/61646/weratedogs-tweets-26.jpg\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdc0f0",
   "metadata": {},
   "source": [
    "#### Summary Report for Data Wrangling Course\n",
    "\n",
    "This was a more targeted course than the previous overview course. The focus was the Data Wrangling process itself and targeted multiple ways to pull data into a Jupyter Notebook project. I learned how to pull the data from csv and tsv files, scraping websites, and through APIs. All of these skills i displayed in the wrangling and cleaning project. \n",
    "\n",
    "Going through the cleaning project was achieved by examining, both visually and programmatically, the various datasets and determining which items were of an actionable quality and which needed some help. I'll admit, it was tricky in some spots and I'm still not 100% certain that I got it all right. I was able to take the newly cleaned and merged table and explore various insights into the data itself.\n",
    "\n",
    "The process for wrangling and cleaning this data was handled in the accepted format (Gather, Assess, Clean). There were a few issues I had with the Twitter API portion of gathering data, primarily because I had never done anything like it before. I was able to get over those hurdles and continue on with the project after what seemed an unacceptably long time. Once the gathering was completed, the process went much smoother and I was able to dive into the data itself. \n",
    "\n",
    "As most of my background is with established datasets, this course has been great for getting myself more acquainted with the data assessment process. Visual inspection is a time consuming process, especially on large datasets. Working through this project programmatically was more insightful than I had thought it would be. \n",
    "\n",
    "Cleaning the data using define, code, test for each issue had its advantages. It allowed me to keep track of each issue and really focus on a single issue at a time. Where I had several, but not all, of the cleanliness issues identified, keeping things in order was invaluable to the iterative process of cleaning while returning to my assessment phase to determine any additional steps in the process.\n",
    "\n",
    "Overall, I am pleased with how the project turned out. I feel that i was able to achieve the requirements and that I learned quite a bit along the way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
